# 梯度下降法


如何降低损失？
(y - y')2 相对于权重和偏差的导数可让我们了解指定样本的损失变化情况
易于计算且为凸形
因此，我们在能够尽可能降低损失的方向上反复采取小步
我们将这些小步称为梯度步长（但它们实际上是负梯度步长）
这种优化策略称为梯度下降法

迈多大步调：超参数 learningRate lr


### SGD 和小批量梯度下降法
可以在每步上计算整个数据集的梯度，但事实证明没有必要这样做
计算小型数据样本的梯度效果很好
每一步抽取一个新的随机样本
**随机梯度下降法(SGD**：一次抽取一个样本
**小批量梯度下降法**：每批包含 10-1000 个样本
损失和梯度在整批范围内达到平衡




### 学习速率（步长：lr）

**损失相对于单个权重的梯度就等于导数**梯度是偏导数的矢量

梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。

例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

**批量**指的是用于在单次迭代中计算梯度的样本总数

**小批量随机梯度下降法（小批量 SGD）**是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

#### 深度学习中经常看到epoch、iteration和batchsize，下面按照自己的理解说说这三个区别：

* （1）batchsize：批大小。在深度学习中，一般采用**SGD**训练，即每次训练在训练集中取batchsize个样本训练； 
* （2）iteration：1个iteration等于使用batchsize个样本训练一次； 
* （3）epoch：1个epoch等于使用训练集中的全部样本训练一次；

举个例子，训练集有1000个样本，batchsize=10，那么： 训练完整个样本集需要： 100次iteration，1次epoch。

### 常用超参数

* steps：训练迭代的总次数。一步计算一批样本产生的损失，然后使用该值修改一次模型的权重。
* batch size：单步的样本数量（随机选择）。例如，SGD 的批次大小为 1。

periods：控制报告的粒度。例如，如果 periods 设为 7 且 steps 设为 70，则练习将每 10 步输出一次损失值（即 7 次）。与超参数不同，我们不希望您修改 periods 的值。请注意，修改 periods 不会更改模型所学习的规律。
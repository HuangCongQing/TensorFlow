{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595993172706",
   "display_name": "Python 3.5.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充\n",
    "```\n",
    "lr = tf.Variable(0.001,dtype = tf.float32)\n",
    "\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)   # lr传到优化器\n",
    "\n",
    " sess.run(tf.assign(lr, 1*(0.98**epoch))) # 迭代一次， lr重新赋值\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracting ../MNIST_data\\train-images-idx3-ubyte.gz\nExtracting ../MNIST_data\\train-labels-idx1-ubyte.gz\nExtracting ../MNIST_data\\t10k-images-idx3-ubyte.gz\nExtracting ../MNIST_data\\t10k-labels-idx1-ubyte.gz\nIter 0 Testing Accuracy: 0.9661,  lr: 0.001\nIter 1 Testing Accuracy: 0.973,  lr: 0.00098\nIter 2 Testing Accuracy: 0.9759,  lr: 0.0009604\nIter 3 Testing Accuracy: 0.9777,  lr: 0.000941192\nIter 4 Testing Accuracy: 0.9791,  lr: 0.00092236814\nIter 5 Testing Accuracy: 0.9787,  lr: 0.0009039208\nIter 6 Testing Accuracy: 0.9785,  lr: 0.00088584237\nIter 7 Testing Accuracy: 0.9813,  lr: 0.0008681255\nIter 8 Testing Accuracy: 0.9814,  lr: 0.000850763\nIter 9 Testing Accuracy: 0.9811,  lr: 0.00083374773\nIter 10 Testing Accuracy: 0.9832,  lr: 0.0008170728\nIter 11 Testing Accuracy: 0.9812,  lr: 0.00080073136\nIter 12 Testing Accuracy: 0.9821,  lr: 0.00078471674\nIter 13 Testing Accuracy: 0.9823,  lr: 0.0007690224\nIter 14 Testing Accuracy: 0.9805,  lr: 0.00075364194\nIter 15 Testing Accuracy: 0.9827,  lr: 0.0007385691\nIter 16 Testing Accuracy: 0.9811,  lr: 0.0007237977\nIter 17 Testing Accuracy: 0.9819,  lr: 0.00070932176\nIter 18 Testing Accuracy: 0.9824,  lr: 0.0006951353\nIter 19 Testing Accuracy: 0.9838,  lr: 0.0006812326\nIter 20 Testing Accuracy: 0.9842,  lr: 0.000667608\n"
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True) \n",
    "# x相对路径  数据集下载到某一目录下（相对路径不懂可自行百度）\n",
    "# one-hot=True 把标签设置为one-hot 格式\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size  # 整除\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # None 100？\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout  多少神经元在工作\n",
    "lr = tf.Variable(0.001,dtype = tf.float32)\n",
    "\n",
    "\n",
    "# 创建神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([1, 2000]))\n",
    "# 激活层\n",
    "layer1 = tf.nn.relu(tf.matmul(x,W1) + b1)\n",
    "# drop层\n",
    "layer1 = tf.nn.dropout(layer1,keep_prob=keep_prob)\n",
    "\n",
    "# 第二层\n",
    "W2 = tf.Variable(tf.truncated_normal([2000,500],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([1, 500]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "layer2 = tf.nn.dropout(layer2,keep_prob=keep_prob)\n",
    "\n",
    "# 第三层\n",
    "W3 = tf.Variable(tf.truncated_normal([500,10],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1,10]))\n",
    "# prediction = tf.nn.softmax(tf.matmul(layer2,W3) + b3)\n",
    "prediction = tf.matmul(layer2,W3) + b3\n",
    "\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction)) \n",
    "# Iter20Testing Accurancy0.9134\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# Iter20Testing Accurancy0.9211\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "# Iter20Testing Accurancy0.9223\n",
    "\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在布尔型列表中  #argmax 返回一维张量中最大值索引\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,axis=1)) # 相同为Ture，argmax 函数起的是1所在维度最大值的index索引  1表示axis第二个维度\n",
    "# 0表示按列查找  1表示按行查找（0列1行）\n",
    "\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 布尔型列表转化为flaot32类型 True变为1.0，False变为0\n",
    "# 求平均值就得到准确率，比如10个值，9个True  1个False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "         sess.run(tf.assign(lr, 0.001*(0.98**epoch))) # 时刻改变 lr\n",
    "         for batch in range(n_batch):\n",
    "             batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 图片和标签\n",
    "             sess.run(train_step, feed_dict={x: batch_xs,y:batch_ys, keep_prob:0.7})\n",
    "         learning_rate = sess.run(lr)\n",
    "         acc = sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0} )\n",
    "         print(\"Iter \" + str(epoch) + \" Testing Accuracy: \" + str(acc)+ \",  lr: \"+ str(learning_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
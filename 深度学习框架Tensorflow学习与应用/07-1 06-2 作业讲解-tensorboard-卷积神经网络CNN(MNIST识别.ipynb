{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596251851954",
   "display_name": "Python 3.5.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充\n",
    "* 权值太多，参数过多，需要样本数量大\n",
    "* 引入局部感受野\n",
    "\n",
    "###  卷积层就是对图像像素进行特征提取，多个不同卷积核产生不同特征图，增加深度，减少参数\n",
    "### 池化层就是降低分辨率，缩小尺寸，减少参数，加快计算\n",
    "\n",
    "### CONV(卷积层-RELU(激活函数)-POOL(池化层)... FC(全连接层) 如下图\n",
    "\n",
    "\n",
    "## tensorboard报错 ：InvalidArgumentError: You must feed a value for placeholder tensor 'inputs/y_input\n",
    "https://github.com/HuangCongQing/TensorFlow/issues/8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://cdn.nlark.com/yuque/0/2019/png/232596/1572371566724-41591b3b-8b49-454d-a4df-d2e3ba33e6cc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进入tensorboard查看\n",
    "```\n",
    "F:  // win系统首先进入所在盘\n",
    "\n",
    "tensorboard --logdir=F:\\Github\\TensorFlow\\深度学习框架Tensorflow学习与应用\\logs\\log-6.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracting ../MNIST_data\\train-images-idx3-ubyte.gz\nExtracting ../MNIST_data\\train-labels-idx1-ubyte.gz\nExtracting ../MNIST_data\\t10k-images-idx3-ubyte.gz\nExtracting ../MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Step: 0, acc: 0.0862\nStep: 10, acc: 0.1287\nStep: 20, acc: 0.1879\nStep: 30, acc: 0.1952\nStep: 40, acc: 0.1712\nStep: 50, acc: 0.1754\nStep: 60, acc: 0.1918\nStep: 70, acc: 0.1514\nStep: 80, acc: 0.2119\nStep: 90, acc: 0.2036\nStep: 100, acc: 0.212\nStep: 110, acc: 0.2961\nStep: 120, acc: 0.3105\nStep: 130, acc: 0.3227\nStep: 140, acc: 0.2691\nStep: 150, acc: 0.3671\nStep: 160, acc: 0.3945\nStep: 170, acc: 0.4187\nStep: 180, acc: 0.4798\nStep: 190, acc: 0.5776\nStep: 200, acc: 0.5821\nStep: 210, acc: 0.6107\nStep: 220, acc: 0.6309\nStep: 230, acc: 0.6714\nStep: 240, acc: 0.6922\nStep: 250, acc: 0.7187\nStep: 260, acc: 0.7277\nStep: 270, acc: 0.7182\nStep: 280, acc: 0.7005\nStep: 290, acc: 0.722\nStep: 300, acc: 0.7339\nStep: 310, acc: 0.7177\nStep: 320, acc: 0.7613\nStep: 330, acc: 0.7285\nStep: 340, acc: 0.7483\nStep: 350, acc: 0.7324\nStep: 360, acc: 0.7528\nStep: 370, acc: 0.752\nStep: 380, acc: 0.757\nStep: 390, acc: 0.7353\nStep: 400, acc: 0.7487\nStep: 410, acc: 0.7627\nStep: 420, acc: 0.7848\nStep: 430, acc: 0.8006\nStep: 440, acc: 0.802\nStep: 450, acc: 0.823\nStep: 460, acc: 0.8293\nStep: 470, acc: 0.8566\nStep: 480, acc: 0.9636\nStep: 490, acc: 0.9675\nStep: 500, acc: 0.9535\nStep: 510, acc: 0.9714\nStep: 520, acc: 0.9717\nStep: 530, acc: 0.9712\nStep: 540, acc: 0.9756\nStep: 550, acc: 0.9771\nStep: 560, acc: 0.9744\nStep: 570, acc: 0.9771\nStep: 580, acc: 0.9779\nStep: 590, acc: 0.9798\nStep: 600, acc: 0.9761\nStep: 610, acc: 0.9806\nStep: 620, acc: 0.9801\nStep: 630, acc: 0.981\nStep: 640, acc: 0.979\nStep: 650, acc: 0.9815\nStep: 660, acc: 0.9798\nStep: 670, acc: 0.9825\nStep: 680, acc: 0.9821\nStep: 690, acc: 0.9819\nStep: 700, acc: 0.9757\nStep: 710, acc: 0.9797\nStep: 720, acc: 0.981\nStep: 730, acc: 0.9778\nStep: 740, acc: 0.9813\nStep: 750, acc: 0.9819\nStep: 760, acc: 0.979\nStep: 770, acc: 0.9826\nStep: 780, acc: 0.9825\nStep: 790, acc: 0.9794\nStep: 800, acc: 0.9808\nStep: 810, acc: 0.9813\nStep: 820, acc: 0.9809\nStep: 830, acc: 0.9831\nStep: 840, acc: 0.983\nStep: 850, acc: 0.9796\nStep: 860, acc: 0.9835\nStep: 870, acc: 0.9829\nStep: 880, acc: 0.981\nStep: 890, acc: 0.9838\nStep: 900, acc: 0.9832\nStep: 910, acc: 0.9867\nStep: 920, acc: 0.9842\nStep: 930, acc: 0.9864\nStep: 940, acc: 0.9836\nStep: 950, acc: 0.9851\nStep: 960, acc: 0.9825\nStep: 970, acc: 0.9839\nStep: 980, acc: 0.9827\nStep: 990, acc: 0.9841\n"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "max_step = 1000\n",
    "keep_ = 0.8\n",
    "log_dir = \"logs/log-6.1\"\n",
    "\n",
    "# 生成权重\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1),name='W')\n",
    "\n",
    "# 生成偏差\n",
    "def bias_vairable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape),name='b')\n",
    "\n",
    "# 记录变量\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME',name='conv2d')\n",
    "        \n",
    "def conv_layer(input_tensor, weight_shape, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable(weight_shape)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_vairable([weight_shape[-1]])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('conv_comput'):\n",
    "            preactivate = conv2d(input_tensor,weights) + biases\n",
    "        with tf.name_scope('activate'):\n",
    "            activations = act(preactivate)\n",
    "        return activations\n",
    "\n",
    "def linear_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_vairable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('linear_comput'):\n",
    "            preactivate = tf.matmul(input_tensor,weights) + biases\n",
    "        with tf.name_scope('activate'):\n",
    "            activations = act(preactivate)\n",
    "        return activations\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Max_pool')\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32,[None,784],name='input_x')\n",
    "    with tf.name_scope('Input_reshape'):\n",
    "        x_image = tf.reshape(x,[-1,28,28,1],name='x-image')\n",
    "        input_summery = tf.summary.image('input',x_image,10)\n",
    "    y = tf.placeholder(tf.float32,[None,10],name='input_y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# 第一次卷积   28*28*1->28*28*32\n",
    "conv_layer1 = conv_layer(x_image,[5,5,1,32],'conv_layer1')\n",
    "# 池化之后变为 14*14*32\n",
    "with tf.name_scope('Max_pool1'):\n",
    "    h_pool1 = max_pool_2x2(conv_layer1)\n",
    "\n",
    "# 第二次卷积 14*14*32->14*14*64\n",
    "conv_layer2 = conv_layer(h_pool1,[5,5,32,64],'conv_layer2')\n",
    "# 第二次池化之后变为 7*7*64\n",
    "with tf.name_scope('Max_pool2'):\n",
    "    h_pool2 = max_pool_2x2(conv_layer2)\n",
    "\n",
    "with tf.name_scope('Flatten'):\n",
    "    flatten_ = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "    \n",
    "# 第一个全连接层 7*7*64 - 1024\n",
    "fc1 = linear_layer(flatten_, 7*7*64, 1024, 'FC1')\n",
    "\n",
    "with tf.name_scope('Dropput'):\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "# 第二个全连接层 1024 - 10\n",
    "logits = linear_layer(fc1_drop, 1024, 10, 'FC2',act=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss_summary = tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "merged = tf.summary.merge([input_summery,loss_summary, accuracy_summary])\n",
    "\n",
    "def get_dict(train):\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        k = keep_\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x:xs, y:ys, keep_prob: k}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "    # train_writer.add_graph(sess.graph)#添加graph图\n",
    "    # test_writer.add_graph(sess.graph)#添加graph图\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(max_step):\n",
    "        if i%10 == 0:\n",
    "            summary,acc = sess.run([merged,accuracy], feed_dict=get_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print(\"Step: \" + str(i) + \", acc: \" + str(acc))\n",
    "        else:\n",
    "            summary,_ = sess.run([merged,train_step], feed_dict=get_dict(True))\n",
    "            train_writer.add_summary(summary,i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.nlark.com/yuque/0/2020/png/232596/1596253070053-454f25f5-7025-49a8-bfb9-65b77e4db986.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}